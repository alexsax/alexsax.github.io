[{"authors":["admin-equal-contrib"],"categories":null,"content":"","date":1630454400,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1630454400,"objectID":"ea3ac6687608b2634d14ddcfaef2fe6b","permalink":"/author/alexander-sax/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/alexander-sax/","section":"authors","summary":"","tags":null,"title":"Alexander Sax*","type":"authors"},{"authors":["admin"],"categories":null,"content":"My research interests are in developing simple models of visual perception. I ultimately want to understand how these systems are shaped by their ecological function. My current research angle is to try to model the relationships between different visual tasks in order to understand some of the general principles. My work so far has been modeling relationships between different computer vision tasks, and then linking those vision tasks to more ecologically realistic tasks in robotics.\nI\u0026rsquo;m currently at UC Berkeley, where I am a PhD student advised by Jitendra Malik. I\u0026rsquo;m also advised by Amir Zamir (@ EPFL). Before coming to Berkeley, I was at Stanford for a M.Sc. (in Computer Science advised by Silvio Savarese and Amir Zamir and B.Sc. (in Math), also from Stanford.\n  ","date":1622505600,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1622505600,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/alexander-sax/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/alexander-sax/","section":"authors","summary":"My research interests are in developing simple models of visual perception. I ultimately want to understand how these systems are shaped by their ecological function. My current research angle is to try to model the relationships between different visual tasks in order to understand some of the general principles.","tags":null,"title":"Alexander Sax","type":"authors"},{"authors":["Alexander Sax*","Ainaz Eftekhar*","Jitendra Malik","Amir Zamir"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1630454400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630454400,"objectID":"53bfd661a6718c84cc1a83e4c3ba3487","permalink":"/publication/omnidata/","publishdate":"2021-09-01T00:00:00Z","relpermalink":"/publication/omnidata/","section":"publication","summary":"The Omnidata annotator is a pipeline to resample comprehensive 3D scans from the real-world into static multi-task vision datasets. Because this is resampling is parametric, we can control or steer datasets. This enables interesting lines of research (such as looking into the effects of these different parameters). And the resampled data can be used to train strong and robust vision models (results, demo).\nFor example, we create a starter dataset of 14 million images sampled from 2000 scanned spaces. Familiar architectures trained on a generated starter dataset reached state-of-the-art performance on multiple common vision tasks and benchmarks, despite having seen no benchmark or non-pipeline data. The depth estimation network outperforms MiDaS and the surface normal estimation network is the first to achieve human-level performance for in-the-wild surface normal estimation (at least according to one metric on the OASIS benchmark).\nWith 3D scanners becoming increasingly prevalent (e.g. on iPhones and iPads), we expect 3D scans to be a rich source of data in the future. We're therefore open-sourcing everything in order to make it easier to do research with steerable datasets. The Dockerized pipeline with CLI and its (mostly Python) code, PyTorch dataloaders for the resulting data, the starter dataset, download scripts, and other utilities are available in the linked GitHub repos above.","tags":["Robustness"],"title":"Omnidata: A Scalable Pipeline for Making Multi-Task Mid-Level Vision Datasets from 3D Scans","type":"publication"},{"authors":["Teresa Yeo*","Oğuzhan Fatih Kar*","Alexander Sax","Amir Zamir"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"92ed760fb1fd9273a45ddb3dc9c18e34","permalink":"/publication/consistency_21/","publishdate":"2021-06-01T00:00:00Z","relpermalink":"/publication/consistency_21/","section":"publication","summary":"We present a method for making neural network predictions robust to shifts from the training data distribution. The proposed method is based on making predictions via a diverse set of cues (called ‘middle domains’) and ensembling them into one strong prediction. The premise of the idea is that predictions made via different cues respond differently to a distribution shift, hence one should be able to merge them into one robust final prediction. We perform the merging in a straightforward but principled manner based on the uncertainty associated with each prediction.\nThe evaluations are performed using multiple tasks and datasets (Taskonomy, Replica, ImageNet, CIFAR) under a wide range of adversarial and non-adversarial distribution shifts which demonstrate the proposed method is considerably more robust than its standard learning counterpart, conventional deep ensembles, and several other baselines.","tags":["Robustness"],"title":"Robustness via Cross-Domain Ensembles","type":"publication"},{"authors":["Alexander Sax*","Bryan Chen*","Francis E. Lewis","Silvio Savarese","Jitendra Malik","Amir Zamir","Lerrel Pinto"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"465ece90175253582c015eb9dbb11e97","permalink":"/publication/midlevel_20/","publishdate":"2020-11-01T00:00:00Z","relpermalink":"/publication/midlevel_20/","section":"publication","summary":"Vision-based robotics often separates the control loop into one module for perception and a separate module for control. It is possible to train the whole system end-to-end (e.g. with deep RL), but doing it 'from scratch' comes with a high sample complexity cost and the final result is often brittle, failing unexpectedly if the test environment differs from that of training.\nWe study the effects of using mid-level visual representations (features learned asynchronously for traditional computer vision objectives), as a generic and easy-to-decode perceptual state in an end-to-end RL framework. Mid-level representations encode invariances about the world, and we show that they aid generalization, improve sample complexity, and lead to a higher final performance. Compared to other approaches for incorporating invariances, such as domain randomization, asynchronously trained mid-level representations scale better: both to harder problems and to larger domain shifts. In practice, this means that mid-level representations could be used to successfully train policies for tasks where domain randomization and learning-from-scratch failed. We report results on both manipulation and navigation tasks, and for navigation include zero-shot sim-to-real experiments on real robots.","tags":["Robustness"],"title":"Robust Policies via Mid-Level Visual Representations","type":"publication"},{"authors":["Alexander Sax*","Amir Zamir*","Teresa Yeo","Oğuzhan Faith Kar","Nikhil Cheerla","Rohan Suri","Zhangjie Cao","Jitendra Malik","Leonidas Guibas"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"9154f63d5c82f0a00b7703a339186e61","permalink":"/publication/consistency/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/publication/consistency/","section":"publication","summary":"Visual perception entails solving a wide set of tasks, e.g., object detection, depth estimation, etc. The predictions made for multiple tasks from the same image are not independent, and therefore, are expected to be 'consistent'. We propose a broadly applicable and fully computational method for augmenting learning with Cross-Task Consistency. The proposed formulation is based on inference-path invariance over a graph of arbitrary tasks. We observe that learning with cross-task consistency leads to more accurate predictions and better generalization to out-of-distribution inputs. This framework also leads to an informative unsupervised quantity, called Consistency Energy, based on measuring the intrinsic consistency of the system. Consistency Energy correlates well with the supervised error (r=0.67), thus it can be employed as an unsupervised confidence metric as well as for detection of out-of-distribution inputs (ROC-AUC=0.95). The evaluations are performed on multiple datasets, including Taskonomy, Replica, CocoDoom, and ApolloScape, and they benchmark cross-task consistency versus various baselines including conventional multi-task learning, cycle consistency, and analytical consistency.","tags":["Robustness"],"title":"Robust Learning Through Cross-Task Consistency","type":"publication"},{"authors":["Jeffrey O. Zhang","Alexander Sax","Amir Zamir","Leonidas Guibas","Jitendra Malik"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1590969600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1590969600,"objectID":"17260a59c098d67b5f5e08df83003626","permalink":"/publication/sidetuning/","publishdate":"2020-08-01T00:00:00Z","relpermalink":"/publication/sidetuning/","section":"publication","summary":"When training a neural network for a desired task, one may prefer to adapt a pre-trained network rather than start with a randomly initialized one -- due to lacking enough training data, performing lifelong learning where the system has to learn a new task while being previously trained for other tasks, or wishing to encode priors in the network via preset weights. The most commonly employed approaches for network adaptation are fine-tuning and using the pre-trained network as a fixed feature extractor, among others. In this paper we propose a straightforward alternative: Side-Tuning. Side-tuning adapts a pre-trained network by training a lightweight ;'side' network that is fused with the (unchanged) pre-trained network using a simple additive process. This simple method works as well as or better than existing solutions while it resolves some of the basic issues with fine-tuning, fixed features, and several other common baselines. In particular, side-tuning is less prone to overfitting when little training data is available, yields better results than using a fixed feature extractor, and does not suffer from catastrophic forgetting in lifelong learning. We demonstrate the performance of side-tuning under a diverse set of scenarios, including lifelong learning (iCIFAR, Taskonomy), reinforcement learning, imitation learning (visual navigation in Habitat), NLP question-answering (SQuAD v2), and single-task transfer learning (Taskonomy), with consistently promising results.","tags":["Robustness"],"title":"Side-Tuning: A Baseline for Network Adaptation via Additive Side Networks","type":"publication"},{"authors":["Alexander Sax","Jeffrey O. Zhang","Amir Zamir","Silvio Savarese","Leonidas Guibas","Jitendra Malik"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1548892800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548892800,"objectID":"5a6caa1c1a1cefccfed4a8975afa899d","permalink":"/publication/midlevel/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/publication/midlevel/","section":"publication","summary":"How much does having **visual priors about the world** (e.g. the fact that the world is 3D) assist in learning to perform **downstream motor tasks** (e.g. delivering a package)? We study this question by integrating a generic perceptual skill set (e.g. a distance estimator, an edge detector, etc.) within a reinforcement learning framework--see Figure 1. This skill set (hereafter **mid-level perception**) provides the policy with a more processed state of the world compared to raw images.  We find that using a mid-level perception confers significant advantages over training end-to-end from scratch (i.e. not leveraging priors) in navigation-oriented tasks. Agents are able to generalize to situations where the from-scratch approach fails and training becomes significantly more sample efficient. However, we show that realizing these gains requires careful selection of the mid-level perceptual skills. Therefore, we refine our findings into an efficient max-coverage feature set that can be adopted in lieu of raw images. We perform our study in completely separate buildings for training and testing and compare against visually blind baseline policies and state-of-the-art feature learning methods.","tags":["Robustness"],"title":"Mid-Level Visual Priors Improve Generalization and Sample Efficiency for Learning Visuomotor Policies","type":"publication"},{"authors":["Fei Xia*","Zhiyang He*","Amir Zamir*","Alexander Sax","Jitendra Malik","Silvio Savarese"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"d53431cfd8597007a408c93ed623e2f6","permalink":"/publication/gibson/","publishdate":"2018-06-01T00:00:00Z","relpermalink":"/publication/gibson/","section":"publication","summary":"Perception and being active (i.e. having a certain level of motion freedom) are closely tied. Learning active perception and sensorimotor control in the physical world is cumbersome as existing algorithms are too slow to efficiently learn in real-time and robots are fragile and costly. This has given rise to learning in simulation which consequently casts a question on transferring to real-world. In this paper, we study learning **perception** for **active agents** in **real-world**, propose a virtual environment for this purpose, and demonstrate complex learned locomotion abilities. The primary characteristics of the learning environments, which transfer into the trained agents, are I) being from the real-world and reflecting its semantic complexity, II) having a mechanism to ensure no need to further domain adaptation prior to deployment of results in real-world, III) embodiment of the agent and making it subject to constraints of space and physics.","tags":["Robustness"],"title":"GibsonEnv: Embodied Real-World Active Perception","type":"publication"},{"authors":["Amir Zamir","Alexander Sax*","William B. Shen*","Jitendra Malik","Leonidas Guibas","Silvio Savarese"],"categories":null,"content":"See the main website for live demos of the API and task-specific networks, as well as pretrained models, samples from the transfer networks, and also explanations of the methodology.\n","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"0a6acb5f4af5692fdc1d701f3b40cfac","permalink":"/publication/taskonomy/","publishdate":"2020-06-01T00:00:00Z","relpermalink":"/publication/taskonomy/","section":"publication","summary":"Would having surface normals simplify the depth estimation of an image?\tDo visual tasks have a relationship, or are they unrelated?\tCommon sense suggests that visual tasks are interdependent, implying the existence of structure among tasks. However, a proper model is needed for the structure to be actionable, e.g., to reduce the supervision required by utilizing task relationships. We therefore ask: which tasks transfer to an arbitrary target task, and how well? Or, how do we learn a set of tasks collectively, with less total supervision? These are some of the questions that can be answered by a computational model of the vision tasks space, as proposed in this paper. We explore the task structure utilizing a sampled dictionary of 2D, 2.5D, 3D, and semantic tasks, and modeling their (1st and higher order) transfer behaviors in a latent space. The product can be viewed as a computational task taxonomy (Taskonomy) and a map of the task space. We study the consequences of this structure, e.g., the emerging task relationships, and exploit them to reduce supervision demand. For instance, we show that the total number of labeled datapoints needed to solve a set of 10 tasks can be reduced to 1/4 while keeping performance nearly the same by using features from multiple proxy tasks. Users can employ a provided Binary Integer Programming solver that leverages the taxonomy to find efficient supervision policies for their own use cases.","tags":["Transfer Learning"],"title":"Taskonomy: Disentangling Task Transfer Learning","type":"publication"},{"authors":["Alexander Sax*","Iro Armeni*","Amir Zamir","Jitendra Malik","Leonidas Guibas","Silvio Savarese"],"categories":null,"content":"The dataset is available using the links above. The code link above provides some utilities for interacting with the dataset in both Python and C++.\n","date":1496275200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1496275200,"objectID":"87016ec42eea78a8fd58eb45128786dc","permalink":"/publication/2d3ds/","publishdate":"2017-06-01T00:00:00Z","relpermalink":"/publication/2d3ds/","section":"publication","summary":"We present a dataset of large-scale indoor spaces that provides a variety of mutually registered modalities from 2D, 2.5D and 3D domains, with instance-level semantic and geometric annotations. The dataset covers over 6,000 m2 and contains over 70,000 RGB images, along with the corresponding depths, surface normals, semantic annotations, global XYZ images (all in forms of both regular and 360◦ equirectangular images) as well as camera information. It also includes registered raw and semantically annotated 3D meshes and point clouds. The dataset enables development of joint and cross-modal learning models and potentially unsupervised approaches utilizing the regularities present in large-scale indoor spaces.","tags":["Robustness"],"title":"2D-3D-Semantic Data for Indoor Scene Understanding","type":"publication"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"}]