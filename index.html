<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Alexander (Sasha) Sax</title>

  <meta name="author" content="Alexander (Sasha) Sax">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">




          <!-- Intro -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Alexander (Sasha) Sax
                  </p>
                  <p>I'm a senior research scientist at <a href="https://ai.meta.com/research/">Meta FAIR</a> in Menlo Park.
                    My research focuses on building <strong>multimodal foundation models</strong> to enable Embodied Agents that can 
                    perceive, act in, and communicate about the <strong>physical world</strong>.

                  </p>
                  <p>
                    I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a
                      href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a> and <a href="https://vilab.epfl.ch/zamir/">Amir Zamir</a> (EPFL).
                    In the summer of 2022, I interned at FAIR with <a href="https://gkioxari.github.io/">Georgia Gkioxari</a>.
                    I got my MS and BS from <a href="https://www.cs.stanford.edu/">Stanford University</a>, where I was advised by <a href="https://www.salesforce.com/blog/author/silvio-savarese/">Silvio Savarese</a> and graduated with an Erd≈ës number of <a href="https://www.csauthors.net/distance/paul-erdos/alexander-sax">3</a>.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:alexandersax@gmail.com">Email</a> &nbsp;/&nbsp;
                    <a href="data/Sax-CV.pdf">CV</a> &nbsp;/&nbsp;
                    <!-- <a href="data/Sax-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                    <a href="https://scholar.google.com/citations?hl=en&user=PIq7jcUAAAAJ">Scholar</a> &nbsp;/&nbsp;
                    <!-- <a href="https://twitter.com/iamsashasax">Twitter</a> &nbsp;/&nbsp; -->
                    <a href="https://github.com/alexsax">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:37%;max-width:37%">
                  <a href="images/headshot2x.png"><img
                      style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo"
                      src="images/headshot2x.png" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>


          <!-- Research -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>Research</h2>
                  <p>
                    <!-- My research focuses on building <strong>foundation models</strong> to enable Embodied Agents that 
                    can perceive, act in, and communicate about the <strong>physical world</strong>.
                    
                    <br>
                    <br> -->
                    A lot of my work is around large-scale training for predictive and generative models using supervised learning and RL.
                    It often involves developing multimodal 3D datasets, simulators, and techniques for efficient pretraining and finetuning.
                    
                    <!-- Some keywords are large-scale pretraining, transfer learning, RL, differentiable rendering, and sim-to-real. -->
                    <!-- It  using techniques like model distillation, RL, sim-to-real, and differentia. -->
                    <!-- Some relevant keywords are large-scale predictive models, transfer learning, RL, sim2real. -->
<!-- 
                    I'm interested in computer vision, deep learning, and embodied agents.
                    Most of my research is about creating .
                    research is about training predictive models to infer and understand the physical world, and 
                    using predictive models to build capable embodied agents.
                    Sometimes I use rendering and physics engines to generate the data. -->
                    Here are some nice honors:
                  </p>
                  <ul style="margin-top: 0px;">
                    <li><i>CVPR Best Paper Award Nomination (<a href="https://consistency.epfl.ch/" style="color: inherit;">X-Task Consistency</a>, 2020)</i></li>
                    <li><i>CVPR Best Paper Award (<a href="http://taskonomy.stanford.edu/" style="color: inherit;">Taskonomy</a>, 2018)</i></li>
                    <li><i>NVIDIA Pioneering Research Award (<a href="http://gibsonenv.stanford.edu/" style="color: inherit;">Gibson Environment</a>, 2018)</i></li>
                    <li><i>Stanford University Distinction in Research (<a href="http://taskonomy.stanford.edu/" style="color: inherit;">Computational Evidence for Structure in the Space of Tasks</a>, 2018)</i></li>
                    <li><i>Winner CVPR Habitat Embodied Agents Challenge (<a href="https://perceptual.actor" style="color: inherit;">Mid-Level Representations</a>, RGB track, 2019)</i></li>
                    <li><i>Outstanding Reviewer (<a href="https://iclr.cc/Conferences/2024/Reviewers">ICLR '24</a>,
                      <a href="https://cvpr.thecvf.com/">CVPR '22</a>)</i></li>
                  </ul>
                  <p>
                    A few papers are <span class="highlight">highlighted</span>. Equal contribution is indicated by <strong class="highlight">*</strong> and listed in alphabetical order.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
         
         
         

          <!-- Papers -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="locate3d_stop()" onmouseover="locate3d_start()" bgcolor="#ffffd0">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center; height: 100%;">
                    <div class="two" id='locate3d_image'><img src="images/locate3d_two.jpg" width=100%></div>
                    <img src='images/locate3d_one.jpg' width=100%>
                  </div>
                  <script type="text/javascript">
                    function locate3d_start() {
                      document.getElementById('locate3d_image').style.opacity = "1";
                    }

                    function locate3d_stop() {
                      document.getElementById('locate3d_image').style.opacity = "0";
                    }
                    locate3d_stop()
                  </script>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://github.com/facebookresearch/locate-3d">
                    <span class="papertitle">Locate 3D: Real-World Object Localization via Self-Supervised Learning in 3D</span>
                  </a>
                  <br>
                  <a href="https://ai.meta.com/research/">The Cortex Team @ FAIR</a>
                  <br>
                  <em>arXiv</em>, 2025
                  <br>
                  <a href="https://locate3d.atmeta.com/">website</a> /
                  <a href="https://locate3d.atmeta.com/demo">demo</a> /
                  <a href="https://arxiv.org/pdf/2504.14151">paper</a> /
                  <a href="https://github.com/facebookresearch/locate-3d">github</a> /
                  <a href="https://huggingface.co/facebook/locate-3d">model</a>
                  <p></p>
                  <p>
                    SotA model for localizing objects in 3D scenes from referring expressions. 
                    The model works directly on sensor observation streams for real-world deployment on robots and AR devices.
                    Includes 3D-JEPA (SSL for sensor point clouds), and introduces the Locate 3D Dataset with over 130K annotations.
                  </p>
                </td>
              </tr>
                            
              <tr onmouseout="fast3r_stop()" onmouseover="fast3r_start()" bgcolor="#ffffd0">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center; height: 100%;">
                    <div class="two" id='fast3r_image'><video width=100% muted autoplay loop>
                        <source src="images/fast3r_two.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video>
                    </div>
                    <img src='images/fast3r_one.jpg' width=100%>
                  </div>
                  <script type="text/javascript">
                    function fast3r_start() {
                      document.getElementById('fast3r_image').style.opacity = "1";
                    }

                    function fast3r_stop() {
                      document.getElementById('fast3r_image').style.opacity = "0";
                    }
                    fast3r_stop()
                  </script>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://fast3r-3d.github.io/">
                    <span class="papertitle">Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward Pass</span>
                  </a>
                  <br>
                  <a href="https://jianningy.github.io/">Jianing Yang</a>,
                  <strong>Alexander Sax</strong>,
                  <a href="https://kevinjliang.github.io/">Kevin J. Liang</a>,
                  <a href="https://mikaelhenaff.github.io/">Mikael Henaff</a>,
                  <a href="https://htang.me/">Hao Tang</a>,
                  <a href="https://www.linkedin.com/in/ang-cao/">Ang Cao</a>,
                  <a href="https://web.eecs.umich.edu/~chai/">Joyce Chai</a>,
                  <a href="https://franziskameier.github.io/">Franziska Meier</a>,
                  <a href="https://www.linkedin.com/in/matt-feiszli/">Matt Feiszli</a>
                  <br>
                  <em>CVPR</em>, 2025
                  <br>
                  <a href="https://fast3r-3d.github.io/">project page</a> /
                  <a href="https://arxiv.org/abs/2501.13928">arXiv</a> /
                  <a href="https://github.com/facebookresearch/fast3r">github</a> /
                  <a href="https://fast3r.ngrok.app/">demo</a>
                  <p></p>
                  <p>
                    Replacing the entire structure-from-motion pipeline with a transformer. A purely learning-based approach for 3D reconstruction that scales well with more data and model parameters.
                  </p>
                </td>
              </tr>

              
              <tr onmouseout="liftgs_stop()" onmouseover="liftgs_start()" bgcolor="#ffffff">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center; height: 100%;">
                    <div class="two" id='liftgs_image'><video width=100% muted autoplay loop>
                        <source src="images/liftgs_two.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video>
                    </div>
                    <img src='images/liftgs_one.jpg' width=100%>
                  </div>
                  <script type="text/javascript">
                    function liftgs_start() {
                      document.getElementById('liftgs_image').style.opacity = "1";
                    }

                    function liftgs_stop() {
                      document.getElementById('liftgs_image').style.opacity = "0";
                    }
                    liftgs_stop()
                  </script>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://liftgs.github.io/">
                    <span class="papertitle">LIFT-GS: Cross-Scene Render-Supervised Distillation for 3D Language Grounding</span>
                  </a>
                  <br>
                  <a href="https://www.linkedin.com/in/ang-cao/">Ang Cao</a>,
                  <a href="https://sergioarnaud.github.io/">Sergio Arnaud</a>,
                  <a href="https://research.facebook.com/people/maksymets-oleksandr/">Oleksandr Maksymets</a>,
                  <a href="https://jianningy.github.io/">Jianing Yang</a>,
                  <a href="https://ayushjain.io/">Ayush Jain</a>,
                  <a href="https://sriramyenamandra.github.io/">Sriram Yenamandra</a>,
                  <a href="">Ada Martin</a>,
                  <a href="https://vincentberges.github.io/">Vincent-Pierre Berges</a>,
                  <a href="https://paulmcvay.github.io/">Paul McVay</a>,
                  <a href="">Ruslan Partsey</a>,
                  <a href="https://aravindr93.github.io/">Aravind Rajeswaran</a>,
                  <a href="https://franziskameier.github.io/">Franziska Meier</a>,
                  <a href="https://web.eecs.umich.edu/~justincj/">Justin Johnson</a>,
                  <a href="https://jjp.cs.umich.edu/">Jeong Joon Park</a>,
                  <strong>Alexander Sax</strong>
                  <br>
                  <em>In submission</em>, 2025
                  <br>
                  <a href="https://liftgs.github.io/">project page</a> /
                  <a href="https://arxiv.org/abs/2502.20389">arXiv</a>
                  <!-- <a href="https://github.com/facebookresearch/liftgs">github (coming soon)</a> -->
                  <p></p>
                  <p>
                    Image-based supervision and differentiable rendering (alone) can train 3D vision-language grounding models. This allows us to distill 3D grounding models from a 2D model.
                  </p>
                </td>
              </tr>


              <tr onmouseout="univlg_stop()" onmouseover="univlg_start()" bgcolor="#ffffff">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center; height: 100%;">
                    <div class="two" id='univlg_image'><img src="images/univlg_two.jpg" width=100%></div>
                    <img src='images/univlg_one.jpg' width=100%>
                  </div>
                  <script type="text/javascript">
                    function univlg_start() {
                      document.getElementById('univlg_image').style.opacity = "1";
                    }

                    function univlg_stop() {
                      document.getElementById('univlg_image').style.opacity = "0";
                    }
                    univlg_stop()
                  </script>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://univlg.github.io/">
                    <span class="papertitle">UniVLG: Unifying 2D and 3D Vision-Language Understanding</span>
                  </a>
                  <br>
                  <a href="https://ayushjain.io/">Ayush Jain*</a>,
                  <a href="https://aswd.github.io/">Alexander Swerdlow*</a>,
                  <a href="">Yuzhou Wang</a>,
                  <a href="https://sergioarnaud.github.io/">Sergio Arnaud</a>,
                  <a href="">Ada Martin</a>,
                  <strong>Alexander Sax</strong>,
                  <a href="https://franziskameier.github.io/">Franziska Meier</a>,
                  <a href="https://www.cs.cmu.edu/~katef/">Katerina Fragkiadaki</a>
                  <br>
                  <em>In submission</em>, 2025
                  <br>
                  <a href="https://univlg.github.io/">project page</a> /
                  <a href="https://arxiv.org/abs/2503.10745">arXiv</a> /
                  <a href="https://github.com/facebookresearch/univlg">github</a>
                  <p></p>
                  <p>
                    A unified architecture for 2D and 3D vision-language understanding that achieves SotA performance in 3D vision-language tasks. It uses 2D pre-training for the encoder and introduces a SAM-like mask decoder that is language-conditioned and shared across modalities.
                  </p>
                </td>
              </tr>


              <tr onmouseout="openeqa_stop()" onmouseover="openeqa_start()" bgcolor="#ffffd0">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center; height: 100%;">
                    <div class="two" id='openeqa_image'><video width=100% muted autoplay loop>
                        <source src="images/openeqa_two.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video>
                    </div>
                    <img src='images/openeqa_one.jpg' width=100%>
                  </div>
                  <script type="text/javascript">
                    function openeqa_start() {
                      document.getElementById('openeqa_image').style.opacity = "1";
                    }

                    function openeqa_stop() {
                      document.getElementById('openeqa_image').style.opacity = "0";
                    }
                    openeqa_stop()
                  </script>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://open-eqa.github.io/">
                    <span class="papertitle">OpenEQA: Embodied Question Answering in the Era of Foundation Models</span>
                  </a>
                  <br>
                  <a href="https://ai.meta.com/research/">The Cortex Team @ FAIR</a>
                  <!-- <a href="https://anuragajay.github.io/">Anurag Ajay*</a>,
                  <a href="https://arjunmajum.github.io/">Arjun Majumdar*</a>,
                  <a href="https://xiaohan-zhang.github.io/">Xiaohan Zhang*</a>,
                  <a href="https://pranavputta.github.io/">Pranav Putta</a>,
                  <a href="https://sriramyenamandra.github.io/">Sriram Yenamandra</a>,
                  <a href="https://mikaelhenaff.github.io/">Mikael Henaff</a>,
                  <a href="https://snehasilwal.github.io/">Sneha Silwal</a>,
                  <a href="https://paulmcvay.github.io/">Paul Mcvay</a>,
                  <a href="https://research.facebook.com/people/maksymets-oleksandr/">Oleksandr Maksymets</a>,
                  <a href="https://sergioarnaud.github.io/">Sergio Arnaud</a>,
                  <a href="https://karmeshyadav.github.io/">Karmesh Yadav</a>,
                  <a href="https://qiyangli.github.io/">Qiyang Li</a>,
                  <a href="https://bennewman.github.io/">Ben Newman</a>,
                  <a href="https://mohitsharma.github.io/">Mohit Sharma</a>,
                  <a href="https://vincentberges.github.io/">Vincent Berges</a>,
                  <a href="https://shiqizhang.github.io/">Shiqi Zhang</a>,
                  <a href="https://people.csail.mit.edu/pulkitag/">Pulkit Agrawal</a>,
                  <a href="https://yonatanbisk.com/">Yonatan Bisk</a>,
                  <a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a>,
                  <a href="https://mrkalakrishnan.github.io/">Mrinal Kalakrishnan</a>,
                  <a href="https://franziskameier.github.io/">Franziska Meier</a>,
                  <a href="https://cpaxton.github.io/">Chris Paxton</a>,
                  <strong>Alexander Sax</strong>,
                  <a href="https://aravindr93.github.io/">Aravind Rajeswaran</a> -->
                  <br>
                  <em>CVPR</em>, 2024
                  <br>
                  <a href="https://open-eqa.github.io/">project page</a> /
                  <a href="https://ai.meta.com/blog/openeqa-embodied-question-answering-robotics-ar-glasses/">blog post</a> /
                  <a href="https://open-eqa.github.io/assets/pdfs/paper.pdf">paper</a> /
                  <a href="https://github.com/facebookresearch/open-eqa">github</a>
                  <p></p>
                  <p>
                    An open-vocabulary benchmark for Embodied Question Answering (EQA) with a new dataset across 180+ real environments.
                    GPT-4V at 48.5% lags behind human performance (85.9%).
                    For questions that require complex spatial understanding, VLMs in December 2024 are nearly "blind". 
                  </p>
                </td>
              </tr>


              <tr onmouseout="omnidata_stop()" onmouseover="omnidata_start()">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center; height: 100%;">
                    <div class="two" id='omnidata_image' style="display: flex; align-items: center;">
                      <video width=100% muted autoplay loop>
                        <source src="images/omnidata_two.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video>
                    </div>
                    <img src='images/omnidata_one.png' width=100%>
                  </div>
                  <script type="text/javascript">
                    function omnidata_start() {
                      document.getElementById('omnidata_image').style.opacity = "1";
                    }

                    function omnidata_stop() {
                      document.getElementById('omnidata_image').style.opacity = "0";
                    }
                    omnidata_stop()
                  </script>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://omnidata.vision/">
                    <span class="papertitle"> Omnidata: A Scalable Pipeline for Making Multi-Task Mid-Level Vision Datasets from 3D Scans</span>
                  </a>
                  <br>
                  <a href="https://ainaz99.github.io/">Ainaz Eftekhar*</a>,
                  <strong>Alexander Sax*</strong>,
                  <a href="https://romanbachmann.ch/">Roman Bachmann</a>,
                  <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>,
                  <a href="https://vilab.epfl.ch/zamir/">Amir Zamir</a>
                  <br>
                  <em>ICCV</em>, 2021
                  <br>
                  <a href="https://omnidata.vision/">project page</a> /
                  <a href="https://arxiv.org/abs/2110.04994">arXiv</a> /
                  <a href="https://github.com/EPFL-VILAB/omnidata">github</a>
                  <p></p>
                  <p>
                    Annotating a large-scale dataset of 3D scanned and artist-generated scenes, using Blender.
                    Transformer models trained for monocular surface normal estimation achieved performance comparable to human annotators on the OASIS benchmark, and 
                    depth models outperformed MidasV2.
                  </p>
                </td>
              </tr>

              <tr onmouseout="crossdomain_stop()" onmouseover="crossdomain_start()">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center; height: 100%;">
                    <div class="two" id='crossdomain_image'><img src="images/cross_domain_two.jpg" width=100%></div>
                    <img src='images/cross_domain_one.jpg' width=100%>
                  </div>
                  <script type="text/javascript">
                    function crossdomain_start() {
                      document.getElementById('crossdomain_image').style.opacity = "1";
                    }

                    function crossdomain_stop() {
                      document.getElementById('crossdomain_image').style.opacity = "0";
                    }
                    crossdomain_stop()
                  </script>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://crossdomain-ensembles.epfl.ch/">
                    <span class="papertitle">Robustness via Cross-Domain Ensembles</span>
                  </a>
                  <br>
                  <a href="https://oguzhankar.github.io/">Oƒüuzhan Fatih Kar*</a>,
                  <a href="https://teresayeo.github.io/">Teresa Yeo*</a>,
                  <strong>Alexander Sax</strong>,
                  <a href="https://vilab.epfl.ch/zamir/">Amir Zamir</a>
                  <br>
                  <em>ICCV</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                  <br>
                  <a href="https://crossdomain-ensembles.epfl.ch/">project page</a> /
                  <a href="https://arxiv.org/abs/2103.10919">arXiv</a> /
                  <a href="https://github.com/EPFL-VILAB/XDEnsembles">github</a>
                  <p></p>
                  <p>
                    A method for creating a robust and diverse ensemble of predictive networks where the average pairwise disagreement provides a measure of uncertainty.
                    The calibration can be supervised with a lightweight post-training step.
                  </p>
                </td>
              </tr>
  
              <tr onmouseout="xtask_stop()" onmouseover="xtask_start()" bgcolor="#ffffd0">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center; height: 100%;">
                    <div class="two" id='xtask_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/xtc_two.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/xtc_one_crop.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function xtask_start() {
                      document.getElementById('xtask_image').style.opacity = "1";
                    }

                    function xtask_stop() {
                      document.getElementById('xtask_image').style.opacity = "0";
                    }
                    xtask_stop()
                  </script>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://consistency.epfl.ch/">
                    <span class="papertitle">Robust Learning Through Cross-Task Consistency
                    </span>
                  </a>
                  <br>
                  <strong>Alexander Sax*</strong>,
                  <a href="https://vilab.epfl.ch/zamir/">Amir Zamir*</a>, 
                  <a href="https://teresayeo.github.io/">Teresa Yeo</a>,
                  <a href="https://oguzhankar.github.io/">Oƒüuzhan Fatih Kar</a>,
                  <a href="https://www.linkedin.com/in/nikhil-cheerla/">Nikhil Cheerla</a>,
                  <a href="https://www.linkedin.com/in/rohan-suri/">Rohan Suri</a>,
                  <a href="https://zhangjie.ca/">Zhangjie Cao</a>,
                  <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>,
                  <a href="https://geometry.stanford.edu/member/guibas/">Leonidas Guibas</a>
                  <br>
                  <em>CVPR</em>, 2020 &nbsp <font color="red"><strong>(Best Paper Award Nominee)</strong></font>
                  <br>
                  <a href="https://consistency.epfl.ch/">project page</a> /
                  <a href="https://arxiv.org/abs/2006.04096">arXiv</a> /
                  <a href="https://github.com/EPFL-VILAB/XTConsistency">github</a>
                  <p>
                    Introducing perceptual consistency losses for large-scale distillation; using learned priors from pretrained networks.
                    We find that monocular priors from depth and surface normal estimation yield sharper details and better generalization than LPIPS.
                  </p>
                </td>
              </tr>



              <tr onmouseout="sidetuning_stop()" onmouseover="sidetuning_start()">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center; height: 100%;">
                    <div class="two" id='sidetuning_image'>
                      <img src="images/sidetune_two.jpg" width=100%>
                    </div>
                    <img src='images/sidetune_one.jpg' width=100%>
                  </div>
                  <script type="text/javascript">
                    function sidetuning_start() {
                      document.getElementById('sidetuning_image').style.opacity = "1";
                    }

                    function sidetuning_stop() {
                      document.getElementById('sidetuning_image').style.opacity = "0";
                    }
                    sidetuning_stop()
                  </script>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://sidetuning.berkeley.edu/">
                    <span class="papertitle">Side-Tuning: A Baseline for Network Adaptation via Additive Side Networks</span>
                  </a>
                  <br>
                  <a href="https://jozhang97.github.io/">Jeffrey O. Zhang</a>,
                  <strong>Alexander Sax</strong>,
                  <a href="http://www.cs.stanford.edu/~amirz/">Amir Zamir</a>,
                  <a href="https://geometry.stanford.edu/member/guibas/">Leonidas Guibas</a>,
                  <a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a>
                  <br>
                  <em>ECCV</em>, 2020
                  <br>
                  <a href="https://sidetuning.berkeley.edu/">project page</a> /
                  <a href="https://arxiv.org/abs/1912.13503">arXiv</a> /
                  <a href="https://github.com/jozhang97/side-tuning">github</a>
                  <p></p>
                  <p>
                    Adding a lightweight side network to modulate a large pretrained network 
                    is surprisingly competitive for network adaptation and lifelong learning. 
                    We show results in vision, language, and robotics using supervised learning and behavior cloning.
                  </p>
                </td>
              </tr>


              <tr onmouseout="midlevel_manipulation_stop()" onmouseover="midlevel_manipulation_start()">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center; height: 100%;">
                    <div class="two" id='midlevel_manipulation_image' style="display: flex; align-items: center;">
                      <img src="images/midlevel_manip_two.gif" width=100%>
                    </div>
                    <img src='images/midlevel_manip_one.jpg' width=100%>
                  </div>
                  <script type="text/javascript">
                    function midlevel_manipulation_start() {
                      document.getElementById('midlevel_manipulation_image').style.opacity = "1";
                    }

                    function midlevel_manipulation_stop() {
                      document.getElementById('midlevel_manipulation_image').style.opacity = "0";
                    }
                    midlevel_manipulation_stop()
                  </script>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://sites.google.com/view/mid-level-representations/home">
                    <span class="papertitle">Robust Policies via Mid-Level Visual Representations: An Experimental Study in Manipulation and Navigation</span>
                  </a>
                  <br>
                  <a href="https://bycn.github.io/">Bryan Chen*</a>,
                  <strong>Alexander Sax*</strong>,
                  <a href="">Francis E. Lewis</a>,
                  <a href="https://www.salesforce.com/blog/author/silvio-savarese/">Silvio Savarese</a>,
                  <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>,
                  <a href="https://vilab.epfl.ch/zamir/">Amir Zamir</a>,
                  <a href="https://www.cs.cmu.edu/~lerrelp/">Lerrel Pinto</a>
                  <br>
                  <em>CoRL</em>, 2020
                  <br>
                  <a href="https://sites.google.com/view/mid-level-representations/home">project page</a> /
                  <a href="https://arxiv.org/abs/2011.06698">arXiv</a> /
                  <a href="https://github.com/alexsax/robust-policies-via-midlevel-vision">github</a>
                  <p></p>
                  <p>
                    In data regimes where visual domain randomization gets near-zero training performance, policies using pretrained image representations can saturate the benchmark.
                    Pretrained representations yield qualitatively better generalization to new environments, including sim-to-real transfer.
                  </p>
                </td>
              </tr>
          

              <tr onmouseout="midlevel_stop()" onmouseover="midlevel_start()">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center; height: 100%;">
                    <div class="two" id='midlevel_image' style="display: flex; align-items: center;">
                      <video width=100% muted autoplay loop>
                        <source src="images/midlevel_two.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video>
                    </div>
                    <img src='images/midlevel_one.jpg' width=100%>
                  </div>
                  <script type="text/javascript">
                    function midlevel_start() {
                      document.getElementById('midlevel_image').style.opacity = "1";
                    }

                    function midlevel_stop() {
                      document.getElementById('midlevel_image').style.opacity = "0";
                    }
                    midlevel_stop()
                  </script>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://perceptual.actor">
                    <span class="papertitle">Learning to Navigate Using Mid-Level Visual Priors</span>
                  </a>
                  <br>
                  <strong>Alexander Sax*</strong>,
                  <a href="https://jozhang97.github.io/">Jeffrey O. Zhang*</a>,
                  <a href="">Bradley Emi</a>,
                  <a href="https://vilab.epfl.ch/zamir/">Amir Zamir</a>,
                  <a href="https://geometry.stanford.edu/member/guibas/">Leonidas Guibas</a>,
                  <a href="https://www.salesforce.com/blog/author/silvio-savarese/">Silvio Savarese</a>,
                  <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
                  <br>
                  <em>CoRL</em>, 2019 &nbsp <font color="red"><strong>(Winner of CVPR19 Habitat Challenge RGB Track)</strong></font>
                  <br>
                  <a href="https://perceptual.actor">project page</a> /
                  <a href="https://arxiv.org/abs/1812.11971">arXiv</a> /
                  <a href="https://github.com/alexsax/midlevel-reps">github</a>
                  <p></p>
                  <p>
                    Training end-to-end policies with pretrained image representations requires 10x less data to achieve a given performance vs. RL from raw pixels.
                    <!-- Features with stronger geometric and segmentation supervision outperformed imagenet features and autoencoders. -->
                  </p>
                </td>
              </tr>



              <tr onmouseout="taskonomy_stop()" onmouseover="taskonomy_start()" bgcolor="#ffffd0">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center; height: 100%;">
                    <div class="two" id='taskonomy_image'>
                      <video width=100% muted autoplay loop>
                        <source src="images/taskonomy_two.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/taskonomy_one.png' width=100%>
                  </div>
                  <script type="text/javascript">
                    function taskonomy_start() {
                      document.getElementById('taskonomy_image').style.opacity = "1";
                    }

                    function taskonomy_stop() {
                      document.getElementById('taskonomy_image').style.opacity = "0";
                    }
                    taskonomy_stop()
                  </script>
                </td> 
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="http://taskonomy.stanford.edu/">
                    <span class="papertitle">Taskonomy: Disentangling Task Transfer Learning</span>
                  </a>
                  <br>
                  <a href="https://vilab.epfl.ch/zamir/">Amir Zamir</a>,
                  <strong>Alexander Sax*</strong>,
                  <a href="">William B. Shen*</a>,
                  <a href="https://geometry.stanford.edu/member/guibas/">Leonidas Guibas</a>,
                  <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>,
                  <a href="https://www.salesforce.com/blog/author/silvio-savarese/">Silvio Savarese</a>
                  <br>
                  <em>CVPR</em>, 2018 &nbsp <font color="red"><strong>(Best Paper Award)</strong></font>
                  <br>
                  <a href="http://taskonomy.stanford.edu/">project page</a> /
                  <a href="https://arxiv.org/abs/1804.08328">arXiv</a> /
                  <a href="https://github.com/StanfordVL/taskonomy">github</a>
                  <p></p>
                  <p>
                    A fully computational approach for modeling the structure across 26 fundamental vision tasks. 
                    Exploiting this structure with a transfer learning curriculum reduces overall supervision by ~2/3.
                  </p>
                </td>
              </tr>

              <tr onmouseout="gibson_stop()" onmouseover="gibson_start()">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='gibson_image'><video width=100% muted autoplay loop>
                        <source src="images/gibson.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/gibson.jpg' width=100%>
                  </div>
                  <script type="text/javascript">
                    function gibson_start() {
                      document.getElementById('gibson_image').style.opacity = "1";
                    }

                    function gibson_stop() {
                      document.getElementById('gibson_image').style.opacity = "0";
                    }
                    gibson_stop()
                  </script>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="http://gibsonenv.stanford.edu/">
                    <span class="papertitle">Gibson Env: Real-World Perception for Embodied Agents</span>
                  </a>
                  <br>
                  <a href="">Zhi-Yang He*</a>,
                  <a href="">Fei Xia*</a>,
                  <a href="https://vilab.epfl.ch/zamir/">Amir Zamir*</a>,
                  <strong>Alexander Sax</strong>,
                  <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>,
                  <a href="https://www.salesforce.com/blog/author/silvio-savarese/">Silvio Savarese</a>
                  <br>
                  <em>CVPR</em>, 2018 &nbsp <font color="red"><strong>(Spotlight, NVIDIA Pioneering Research Award)</strong></font>
                  <br>
                  <a href="http://gibsonenv.stanford.edu/">project page</a> /
                  <a href="https://arxiv.org/abs/1808.10654">arXiv</a> /
                  <a href="https://github.com/StanfordVL/GibsonEnv">github</a>
                  <p></p>
                  <p>
                    A rendering and physics simulator to bridge the gap between large-scale simulation and real-world environments.
                    The simulator features a database of thousands of real spaces and uses a neural network for infilling, like an early form of <a href="https://en.wikipedia.org/wiki/Deep_learning_super_sampling">DLSS</a>.
                  </p>
                </td>
              </tr>


              <tr onmouseout="semantics2d3d_stop()" onmouseover="semantics2d3d_start()">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center; height: 100%;">
                    <div class="two" id='semantics2d3d_image'><img src="images/2d3ds_two.jpg" width=100%></div>
                    <img src='images/2d3ds_one.jpg' width=100%>
                  </div>
                  <script type="text/javascript">
                    function semantics2d3d_start() {
                      document.getElementById('semantics2d3d_image').style.opacity = "1";
                    }

                    function semantics2d3d_stop() {
                      document.getElementById('semantics2d3d_image').style.opacity = "0";
                    }
                    semantics2d3d_stop()
                  </script>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://3dsemantics.stanford.edu/">
                    <span class="papertitle">Joint 2D-3D-Semantic Data for Indoor Scene Understanding</span>
                  </a>
                  <br>
                  <a href="">Iro Armeni*</a>,
                  <strong>Alexander Sax*</strong>,
                  <a href="https://vilab.epfl.ch/zamir/">Amir R. Zamir</a>,
                  <a href="https://www.salesforce.com/blog/author/silvio-savarese/">Silvio Savarese</a>
                  <br>
                  <em>arXiv</em>, 2017
                  <br>
                  <a href="https://github.com/alexsax/2D-3D-Semantics">project page</a> /
                  <a href="https://arxiv.org/abs/1702.01105">arXiv</a> /
                  <a href="https://github.com/alexsax/2D-3D-Semantics">github</a>
                  <p></p>
                  <p>
                    A large-scale indoor dataset providing mutually registered 2D, 2.5D and 3D modalities with instance-level semantic annotations. 
                    Covers over 6,000m¬≤ across 6 large-scale indoor areas with 70,000+ RGB images and corresponding depths, surface normals, semantic annotations, and global XYZ coordinates.
                  </p>
                </td>
              </tr>

            </tbody>
          </table>


          <table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;">
            <tbody>
              <tr>
                <td>
                  <h2>Misc.</h2>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

<!-- 
              <tr>
                <td align="center" style="padding:16px;width:20%;vertical-align:middle">
                  <div class="colored-box" style="background-color: #fcb97d;">
                    <h2>Micropapers</h2>
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                  <br>
                  <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain
                    Functions</a>
                  <br>
                  <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                  <br>
                  <a href="https://jonbarron.info/data/cvpr2023_llm_workshop_annotated.pdf">Scholars & Big Models: How
                    Can Academics Adapt?</a>
                </td>
              </tr> -->


              <!-- <tr>
                <td align="center" style="padding:16px;width:20%;vertical-align:middle">
                  <div class="colored-box" style="background-color: #aaba9e;">
                    <h2>Recorded Talks</h2>
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:center">
                  <a href="https://www.youtube.com/watch?v=h9vq_65eDas">View Dependent Podcast, 2024</a>
                  <br>
                  <a href="https://www.youtube.com/watch?v=4tDhYsFuEqo">Bay Area Robotics Symposium, 2023
                  </a>
                  <br>
                  <a href="https://youtu.be/TvWkwDYtBP4?t=7604">EGSR Keynote, 2021</a>
                  <br>
                  <a href="https://www.youtube.com/watch?v=nRyOzHpcr4Q">TUM AI Lecture Series, 2020</a>
                  <br>
                  <a href="https://www.youtube.com/watch?v=HfJpQCBTqZs">Vision & Graphics Seminar at MIT, 2020</a>
                </td>
              </tr> -->

              <tr>
                <td align="center" style="padding:16px;width:20%;vertical-align:middle">
                  <div class="colored-box" style="background-color: #c6b89e;">
                    <h2>Academic Service</h2>
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:center">
                  <a href="https://bair.berkeley.edu/">Graduate Mentor: BAIR Undergraduate Mentoring,	2019-2023</a>
                  <br>
                  <a href="">Graduate Admissions (Berkeley 2x, EPFL 1x)</a>
                  <br>
                  <a href="http://3dv.stanford.edu/people.html">Student Organizer, 3DV 2016</a>
                  <br>
                  <a href="https://www.assu.stanford.edu/branches/class-presidents">Stanford Junior Class President, 2014-2015</a>
                  <!-- <br>
                  <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
                  <br>
                  <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Award Committee Member, CVPR 2021</a>
                  <br>
                  <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a> -->
                  <br>
                </td>
              </tr>



              <tr>
                <td align="center" style="padding:16px;width:20%;vertical-align:middle">
                  <div class="colored-box" style="background-color: #edd892;">
                    <h2>Teaching</h2>
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:center">
                  <a href="https://eecs189.org/">Graduate Student Instructor,
                    Berkeley CS 189/289A 2020 (Machine Learning)</a>
                  <br>
                  <a href="http://inst.eecs.berkeley.edu/~cs188/fa10/announcements.html">Head TA,
                    Stanford CS331B (Representation Learning) Fall 2018</a>
                  <br>
                  <a href="https://web.stanford.edu/class/cs103/">Teaching Assistant, 
                    Stanford CS103 (Mathematical Foundations of Computing) 2015</a>
                </td>
              </tr>




            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody> 
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    Thanks <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a> for the clean website.
                    <br>
                    Last updated December 2024.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>