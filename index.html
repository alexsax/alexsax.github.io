<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Alexander (Sasha) Sax</title>

  <meta name="author" content="Alexander (Sasha) Sax">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">

</head>

<body>
  <table
    style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">




          <!-- Intro -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p class="name" style="text-align: center;">
                    Alexander (Sasha) Sax
                  </p>
                  <p>I am a senior research scientist at <a href="https://ai.meta.com/research/">Meta Superintelligence Labs</a> working on the intersection of multimodal models and embodied AI.
                    <br><br>
                    <!-- Frontier models increasingly master natural language, images, video generation, and coding, but lack <strong>long-horizon spatial understanding</strong>.  -->
                    My research currently focuses on equipping models with <strong>spatial reasoning and memory</strong>, 
                    and scaling up to train agents that can not only perceive their physical environment but actively reason and act within it.
                  </p>
                  <p>
                    I did my PhD at <a href="http://www.eecs.berkeley.edu/">UC Berkeley</a>, where I was advised by <a
                      href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a> and <a href="https://vilab.epfl.ch/zamir/">Amir Zamir</a> (EPFL).
                    In the summer of 2022, I interned at FAIR with <a href="https://gkioxari.github.io/">Georgia Gkioxari</a>.
                    I got my MS and BS from <a href="https://www.cs.stanford.edu/">Stanford University</a>, where I was advised by <a href="https://www.salesforce.com/blog/author/silvio-savarese/">Silvio Savarese</a> and graduated with an Erdős number of <a href="https://www.csauthors.net/distance/paul-erdos/alexander-sax">3</a>.
                  </p>
                  <p style="text-align:center">
                    <a href="mailto:alexandersax@gmail.com">Email</a> &nbsp;/&nbsp;
                    <a href="data/Sax-CV.pdf">CV</a> &nbsp;/&nbsp;
                    <!-- <a href="data/Sax-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                    <a href="https://scholar.google.com/citations?hl=en&user=PIq7jcUAAAAJ">Scholar</a> &nbsp;/&nbsp;
                    <!-- <a href="https://twitter.com/iamsashasax">Twitter</a> &nbsp;/&nbsp; -->
                    <a href="https://github.com/alexsax">Github</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:37%;max-width:37%">
                  <a href="images/headshot2x.png"><img
                      style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo"
                      src="images/headshot2x.png" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>


          <!-- Research -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:16px;width:100%;vertical-align:middle">
                  <h2>Research</h2>
                  <p>
                    My recent work focuses on creating new model capabilities for understanding, grounding, and reconstructing the physical world.
                    This work advances the full training and data generation loop, creating scalable techniques for pretraining and post-training multimodal models.
                    <br><br>
                    Here are some nice honors:
                  </p>
                  <ul style="margin-top: 0px;">
                    <li><i>CVPR Best Paper Award Nomination (<a href="https://consistency.epfl.ch/" style="color: inherit;">X-Task Consistency</a>, 2020)</i></li>
                    <li><i>CVPR Best Paper Award (<a href="http://taskonomy.stanford.edu/" style="color: inherit;">Taskonomy</a>, 2018)</i></li>
                    <li><i>NVIDIA Pioneering Research Award (<a href="http://gibsonenv.stanford.edu/" style="color: inherit;">Gibson Environment</a>, 2018)</i></li>
                    <li><i>Stanford University Distinction in Research (<a href="http://taskonomy.stanford.edu/" style="color: inherit;">Computational Evidence for Structure in the Space of Tasks</a>, 2018)</i></li>
                    <li><i>Winner CVPR Habitat Embodied Agents Challenge (<a href="https://perceptual.actor" style="color: inherit;">Mid-Level Representations</a>, RGB track, 2019)</i></li>
                    <li><i>Outstanding Reviewer (<a href="https://iclr.cc/Conferences/2024/Reviewers">ICLR '24</a>,
                      <a href="https://cvpr.thecvf.com/">CVPR '22</a>)</i></li>
                  </ul>
                  <p>
                    A few papers are <span class="highlight">highlighted</span>. Equal contribution is indicated by <strong class="highlight">*</strong> and listed in alphabetical order.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
         
         
         

          <!-- Papers -->
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

              <tr onmouseout="sam3d_stop()" onmouseover="sam3d_start()" bgcolor="#ffffd0">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center; height: 100%;">
                    <div class="two" id='sam3d_image'><video width=100% muted autoplay loop>
                        <source src="images/sam3d_two.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video>
                    </div>
                    <img src='images/sam3d_one.jpg' width=100%>
                  </div>
                  <script type="text/javascript">
                    function sam3d_start() {
                      document.getElementById('sam3d_image').style.opacity = "1";
                    }

                    function sam3d_stop() {
                      document.getElementById('sam3d_image').style.opacity = "0";
                    }
                    sam3d_stop()
                  </script>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://github.com/facebookresearch/sam-3d-objects">
                    <span class="papertitle">SAM 3D: 3Dfy Anything in Images</span>
                  </a>
                  <br>
                  <a href="https://ai.meta.com/research/">Meta Superintelligence Labs</a>
                  <br>
                  <em>arXiv</em>, 2025
                  <br>
                  <a href="https://ai.meta.com/sam3d/">project page</a> /
                  <a href="https://aidemos.meta.com/segment-anything/editor/convert-image-to-3d">demo</a> /
                  <a href="https://arxiv.org/abs/2511.16624">paper</a> /
                  <a href="https://github.com/facebookresearch/sam-3d-objects">github</a>
                  <p></p>
                  <p>
                    A foundation model for reconstructing objects' shape, pose, and texture from a single image.
                    We cracked the 3D data bottleneck by having humans rank model generations.
                    <!-- Pretraining on synthetic data, then using human feedback for sim-to-real transfer and preference alignment. -->
                    After pretraining on synthetic data, we use human feedback for sim-to-real transfer and preference alignment.

                    <!-- And we demonstrate this to train a foundational model for visually grounded 3D object reconstruction. <br> -->
                  </p>
                </td>
              </tr>

              <tr onmouseout="fast3r_stop()" onmouseover="fast3r_start()" bgcolor="#ffffff">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center; height: 100%;">
                    <div class="two" id='fast3r_image'><video width=100% muted autoplay loop>
                        <source src="images/fast3r_two.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video>
                    </div>
                    <img src='images/fast3r_one.jpg' width=100%>
                  </div>
                  <script type="text/javascript">
                    function fast3r_start() {
                      document.getElementById('fast3r_image').style.opacity = "1";
                    }

                    function fast3r_stop() {
                      document.getElementById('fast3r_image').style.opacity = "0";
                    }
                    fast3r_stop()
                  </script>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://fast3r-3d.github.io/">
                    <span class="papertitle">Fast3R: Towards 3D Reconstruction of 1000+ Images in One Forward Pass</span>
                  </a>
                  <br>
                  <a href="https://jianningy.github.io/">Jianing Yang</a>,
                  <strong>Alexander Sax</strong>,
                  <a href="https://kevinjliang.github.io/">Kevin J. Liang</a>,
                  <a href="https://mikaelhenaff.github.io/">Mikael Henaff</a>,
                  <a href="https://htang.me/">Hao Tang</a>,
                  <a href="https://www.linkedin.com/in/ang-cao/">Ang Cao</a>,
                  <a href="https://web.eecs.umich.edu/~chai/">Joyce Chai</a>,
                  <a href="https://franziskameier.github.io/">Franziska Meier</a>,
                  <a href="https://www.linkedin.com/in/matt-feiszli/">Matt Feiszli</a>
                  <br>
                  <em>CVPR</em>, 2025
                  <br>
                  <a href="https://fast3r-3d.github.io/">project page</a> /
                  <a href="https://fast3r.ngrok.app/">demo</a> /
                  <a href="https://arxiv.org/abs/2501.13928">paper</a> /
                  <a href="https://github.com/facebookresearch/fast3r">github</a>
                  <p></p>
                  <p>
                    Replace the entire structure-from-motion pipeline with a single transformer. 
                    A purely learning-based approach for 3D reconstruction that scales with more data and compute.
                  </p>
                </td>
              </tr>

              <tr onmouseout="liftgs_stop()" onmouseover="liftgs_start()" bgcolor="#ffffff">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center; height: 100%;">
                    <div class="two" id='liftgs_image'><video width=100% muted autoplay loop>
                        <source src="images/liftgs_two.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video>
                    </div>
                    <img src='images/liftgs_one.jpg' width=100%>
                  </div>
                  <script type="text/javascript">
                    function liftgs_start() {
                      document.getElementById('liftgs_image').style.opacity = "1";
                    }

                    function liftgs_stop() {
                      document.getElementById('liftgs_image').style.opacity = "0";
                    }
                    liftgs_stop()
                  </script>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://liftgs.github.io/">
                    <span class="papertitle">From Thousands to Billions: 3D Visual Language Grounding via Render-Supervised Distillation from 2D VLMs</span>
                  </a>
                  <br>
                  <a href="https://www.linkedin.com/in/ang-cao/">Ang Cao</a>,
                  <a href="https://sergioarnaud.github.io/">Sergio Arnaud</a>,
                  <a href="https://research.facebook.com/people/maksymets-oleksandr/">Oleksandr Maksymets</a>,
                  <a href="https://jianningy.github.io/">Jianing Yang</a>,
                  <a href="https://ayushjain.io/">Ayush Jain</a>,
                  <a href="https://sriramyenamandra.github.io/">Sriram Yenamandra</a>,
                  <a href="">Ada Martin</a>,
                  <a href="https://vincentberges.github.io/">Vincent-Pierre Berges</a>,
                  <a href="https://paulmcvay.github.io/">Paul McVay</a>,
                  <a href="">Ruslan Partsey</a>,
                  <a href="https://aravindr93.github.io/">Aravind Rajeswaran</a>,
                  <a href="https://franziskameier.github.io/">Franziska Meier</a>,
                  <a href="https://web.eecs.umich.edu/~justincj/">Justin Johnson</a>,
                  <a href="https://jjp.cs.umich.edu/">Jeong Joon Park</a>,
                  <strong>Alexander Sax</strong>
                  <br>
                  <em>ICML</em>, 2025
                  <br>
                  <a href="https://liftgs.github.io/">project page</a> /
                  <a href="https://arxiv.org/abs/2502.20389">arXiv</a>
                  <!-- <a href="https://github.com/facebookresearch/liftgs">github (coming soon)</a> -->
                  <p></p>
                  <p>
                    Use differentiable rendering to supervise 3D masks.

                    We distill from 2D VLMs (think: <a href="https://ai.meta.com/sam3/">SAM 3</a> or <a href="https://ai.google.dev/gemini-api/docs/image-understanding#capabilities">Gemini 3</a>), then finetune on ~50k samples for 3D vision-language grounding. Achieves SOTA performance with strong empirical data scaling.
                  </p>
                </td>
              </tr>

              <tr onmouseout="locate3d_stop()" onmouseover="locate3d_start()" bgcolor="#ffffff">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center; height: 100%;">
                    <div class="two" id='locate3d_image'><img src="images/locate3d_two.jpg" width=100%></div>
                    <img src='images/locate3d_one.jpg' width=100%>
                  </div>
                  <script type="text/javascript">
                    function locate3d_start() {
                      document.getElementById('locate3d_image').style.opacity = "1";
                    }

                    function locate3d_stop() {
                      document.getElementById('locate3d_image').style.opacity = "0";
                    }
                    locate3d_stop()
                  </script>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://github.com/facebookresearch/locate-3d">
                    <span class="papertitle">Locate 3D: Real-World Object Localization via Self-Supervised Learning in 3D</span>
                  </a>
                  <br>
                  <a href="https://ai.meta.com/research/">The Cortex Team @ FAIR</a>
                  <br>
                  <em>ICML</em>, 2025 &nbsp <font color="red"><strong>(Spotlight)</strong></font>
                  <br>
                  <a href="https://locate3d.atmeta.com/">website</a> /
                  <a href="https://locate3d.atmeta.com/demo">demo</a> /
                  <a href="https://arxiv.org/pdf/2504.14151">paper</a> /
                  <a href="https://github.com/facebookresearch/locate-3d">github</a> /
                  <a href="https://huggingface.co/facebook/locate-3d">model</a>
                  <p></p>
                  <p>
                    Use natural langauge to localize objects in 3D scenes.
                    Combines a post-training data engine for 3D language grounding with 3D-JEPA pretraining.
                  </p>
                </td>
              </tr>


              <tr onmouseout="univlg_stop()" onmouseover="univlg_start()" bgcolor="#ffffff">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center; height: 100%;">
                    <div class="two" id='univlg_image'><img src="images/univlg_two.jpg" width=100%></div>
                    <img src='images/univlg_one.jpg' width=100%>
                  </div>
                  <script type="text/javascript">
                    function univlg_start() {
                      document.getElementById('univlg_image').style.opacity = "1";
                    }

                    function univlg_stop() {
                      document.getElementById('univlg_image').style.opacity = "0";
                    }
                    univlg_stop()
                  </script>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://univlg.github.io/">
                    <span class="papertitle">UniVLG: Unifying 2D and 3D Vision-Language Understanding</span>
                  </a>
                  <br>
                  <a href="https://ayushjain.io/">Ayush Jain*</a>,
                  <a href="https://aswd.github.io/">Alexander Swerdlow*</a>,
                  <a href="">Yuzhou Wang</a>,
                  <a href="https://sergioarnaud.github.io/">Sergio Arnaud</a>,
                  <a href="">Ada Martin</a>,
                  <strong>Alexander Sax</strong>,
                  <a href="https://franziskameier.github.io/">Franziska Meier</a>,
                  <a href="https://www.cs.cmu.edu/~katef/">Katerina Fragkiadaki</a>
                  <br>
                  <em>ICML</em>, 2025
                  <br>
                  <a href="https://univlg.github.io/">project page</a> /
                  <a href="https://arxiv.org/abs/2503.10745">arXiv</a> /
                  <a href="https://github.com/facebookresearch/univlg">github</a>
                  <p></p>
                  <p>
                    A unified architecture for joint 2D and 3D vision-language understanding that achieves SotA performance in 3D language grounding benchmarks.
                  </p>
                </td>
              </tr>


              <tr onmouseout="openeqa_stop()" onmouseover="openeqa_start()" bgcolor="#ffffd0">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center; height: 100%;">
                    <div class="two" id='openeqa_image'><video width=100% muted autoplay loop>
                        <source src="images/openeqa_two.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video>
                    </div>
                    <img src='images/openeqa_one.jpg' width=100%>
                  </div>
                  <script type="text/javascript">
                    function openeqa_start() {
                      document.getElementById('openeqa_image').style.opacity = "1";
                    }

                    function openeqa_stop() {
                      document.getElementById('openeqa_image').style.opacity = "0";
                    }
                    openeqa_stop()
                  </script>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://open-eqa.github.io/">
                    <span class="papertitle">OpenEQA: Embodied Question Answering in the Era of Foundation Models</span>
                  </a>
                  <br>
                  <a href="https://ai.meta.com/research/">The Cortex Team @ FAIR</a>
                  <!-- <a href="https://anuragajay.github.io/">Anurag Ajay*</a>,
                  <a href="https://arjunmajum.github.io/">Arjun Majumdar*</a>,
                  <a href="https://xiaohan-zhang.github.io/">Xiaohan Zhang*</a>,
                  <a href="https://pranavputta.github.io/">Pranav Putta</a>,
                  <a href="https://sriramyenamandra.github.io/">Sriram Yenamandra</a>,
                  <a href="https://mikaelhenaff.github.io/">Mikael Henaff</a>,
                  <a href="https://snehasilwal.github.io/">Sneha Silwal</a>,
                  <a href="https://paulmcvay.github.io/">Paul Mcvay</a>,
                  <a href="https://research.facebook.com/people/maksymets-oleksandr/">Oleksandr Maksymets</a>,
                  <a href="https://sergioarnaud.github.io/">Sergio Arnaud</a>,
                  <a href="https://karmeshyadav.github.io/">Karmesh Yadav</a>,
                  <a href="https://qiyangli.github.io/">Qiyang Li</a>,
                  <a href="https://bennewman.github.io/">Ben Newman</a>,
                  <a href="https://mohitsharma.github.io/">Mohit Sharma</a>,
                  <a href="https://vincentberges.github.io/">Vincent Berges</a>,
                  <a href="https://shiqizhang.github.io/">Shiqi Zhang</a>,
                  <a href="https://people.csail.mit.edu/pulkitag/">Pulkit Agrawal</a>,
                  <a href="https://yonatanbisk.com/">Yonatan Bisk</a>,
                  <a href="https://www.cc.gatech.edu/~dbatra/">Dhruv Batra</a>,
                  <a href="https://mrkalakrishnan.github.io/">Mrinal Kalakrishnan</a>,
                  <a href="https://franziskameier.github.io/">Franziska Meier</a>,
                  <a href="https://cpaxton.github.io/">Chris Paxton</a>,
                  <strong>Alexander Sax</strong>,
                  <a href="https://aravindr93.github.io/">Aravind Rajeswaran</a> -->
                  <br>
                  <em>CVPR</em>, 2024
                  <br>
                  <a href="https://open-eqa.github.io/">project page</a> /
                  <a href="https://ai.meta.com/blog/openeqa-embodied-question-answering-robotics-ar-glasses/">blog post</a> /
                  <a href="https://open-eqa.github.io/assets/pdfs/paper.pdf">paper</a> /
                  <a href="https://github.com/facebookresearch/open-eqa">github</a>
                  <p></p>
                  <p>
                    An open-vocabulary benchmark for Embodied Question Answering across 180+ real-world scenes.
                    Humans far outperform VLMs on tasks that require complex spatial understanding.
                  </p>
                </td>
              </tr>


              <tr onmouseout="omnidata_stop()" onmouseover="omnidata_start()" bgcolor="#ffffd0">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center; height: 100%;">
                    <div class="two" id='omnidata_image' style="display: flex; align-items: center;">
                      <video width=100% muted autoplay loop>
                        <source src="images/omnidata_two.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video>
                    </div>
                    <img src='images/omnidata_one.png' width=100%>
                  </div>
                  <script type="text/javascript">
                    function omnidata_start() {
                      document.getElementById('omnidata_image').style.opacity = "1";
                    }

                    function omnidata_stop() {
                      document.getElementById('omnidata_image').style.opacity = "0";
                    }
                    omnidata_stop()
                  </script>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://omnidata.vision/">
                    <span class="papertitle"> Omnidata: A Scalable Pipeline for Making Multi-Task Mid-Level Vision Datasets from 3D Scans</span>
                  </a>
                  <br>
                  <a href="https://ainaz99.github.io/">Ainaz Eftekhar*</a>,
                  <strong>Alexander Sax*</strong>,
                  <a href="https://romanbachmann.ch/">Roman Bachmann</a>,
                  <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>,
                  <a href="https://vilab.epfl.ch/zamir/">Amir Zamir</a>
                  <br>
                  <em>ICCV</em>, 2021
                  <br>
                  <a href="https://omnidata.vision/">project page</a> /
                  <a href="https://arxiv.org/abs/2110.04994">arXiv</a> /
                  <a href="https://github.com/EPFL-VILAB/omnidata">github</a>
                  <p></p>
                  <p>
                    A multimodal 2D/3D dataset of millions of frames from thousands of scanned and synthetic scenes.
                    For surface normal estimation, ViTs trained on Omnidata achieve human-level performance on OASIS. 
                    SOTA depth and the annotation engine is released as OSS.
                  </p>
                </td>
              </tr>

              <tr onmouseout="crossdomain_stop()" onmouseover="crossdomain_start()">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center; height: 100%;">
                    <div class="two" id='crossdomain_image'><img src="images/cross_domain_two.jpg" width=100%></div>
                    <img src='images/cross_domain_one.jpg' width=100%>
                  </div>
                  <script type="text/javascript">
                    function crossdomain_start() {
                      document.getElementById('crossdomain_image').style.opacity = "1";
                    }

                    function crossdomain_stop() {
                      document.getElementById('crossdomain_image').style.opacity = "0";
                    }
                    crossdomain_stop()
                  </script>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://crossdomain-ensembles.epfl.ch/">
                    <span class="papertitle">Robustness via Cross-Domain Ensembles</span>
                  </a>
                  <br>
                  <a href="https://oguzhankar.github.io/">Oğuzhan Fatih Kar*</a>,
                  <a href="https://teresayeo.github.io/">Teresa Yeo*</a>,
                  <strong>Alexander Sax</strong>,
                  <a href="https://vilab.epfl.ch/zamir/">Amir Zamir</a>
                  <br>
                  <em>ICCV</em>, 2021 &nbsp <font color="red"><strong>(Oral Presentation)</strong></font>
                  <br>
                  <a href="https://crossdomain-ensembles.epfl.ch/">project page</a> /
                  <a href="https://arxiv.org/abs/2103.10919">arXiv</a> /
                  <a href="https://github.com/EPFL-VILAB/XDEnsembles">github</a>
                  <p></p>
                  <p>
                    Joint uncertainty estimation for perception tasks. Calibration is supervised with a lightweight post-training step.
                  </p>
                </td>
              </tr>
  
              <tr onmouseout="xtask_stop()" onmouseover="xtask_start()" bgcolor="#ffffd0">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center; height: 100%;">
                    <div class="two" id='xtask_image'><video width=100% height=100% muted autoplay loop>
                        <source src="images/xtc_two.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/xtc_one_crop.png' width="160">
                  </div>
                  <script type="text/javascript">
                    function xtask_start() {
                      document.getElementById('xtask_image').style.opacity = "1";
                    }

                    function xtask_stop() {
                      document.getElementById('xtask_image').style.opacity = "0";
                    }
                    xtask_stop()
                  </script>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://consistency.epfl.ch/">
                    <span class="papertitle">Robust Learning Through Cross-Task Consistency
                    </span>
                  </a>
                  <br>
                  <strong>Alexander Sax*</strong>,
                  <a href="https://vilab.epfl.ch/zamir/">Amir Zamir*</a>, 
                  <a href="https://teresayeo.github.io/">Teresa Yeo</a>,
                  <a href="https://oguzhankar.github.io/">Oğuzhan Fatih Kar</a>,
                  <a href="https://www.linkedin.com/in/nikhil-cheerla/">Nikhil Cheerla</a>,
                  <a href="https://www.linkedin.com/in/rohan-suri/">Rohan Suri</a>,
                  <a href="https://zhangjie.ca/">Zhangjie Cao</a>,
                  <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>,
                  <a href="https://geometry.stanford.edu/member/guibas/">Leonidas Guibas</a>
                  <br>
                  <em>CVPR</em>, 2020 &nbsp <font color="red"><strong>(Best Paper Award Nominee)</strong></font>
                  <br>
                  <a href="https://consistency.epfl.ch/">project page</a> /
                  <a href="https://arxiv.org/abs/2006.04096">arXiv</a> /
                  <a href="https://github.com/EPFL-VILAB/XTConsistency">github</a>
                  <p>
                    Large-scale analysis of pretrained image-to-image networks.
                    Compared to purely semantic perceptual losses like LPIPS, geometric perceptual consistency losses (depth/surface normal) yield sharper details and improve generalization to new domains.
                  </p>
                </td>
              </tr>



              <tr onmouseout="sidetuning_stop()" onmouseover="sidetuning_start()">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center; height: 100%;">
                    <div class="two" id='sidetuning_image'>
                      <img src="images/sidetune_two.jpg" width=100%>
                    </div>
                    <img src='images/sidetune_one.jpg' width=100%>
                  </div>
                  <script type="text/javascript">
                    function sidetuning_start() {
                      document.getElementById('sidetuning_image').style.opacity = "1";
                    }

                    function sidetuning_stop() {
                      document.getElementById('sidetuning_image').style.opacity = "0";
                    }
                    sidetuning_stop()
                  </script>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://sidetuning.berkeley.edu/">
                    <span class="papertitle">Side-Tuning: A Baseline for Network Adaptation via Additive Side Networks</span>
                  </a>
                  <br>
                  <a href="https://jozhang97.github.io/">Jeffrey O. Zhang</a>,
                  <strong>Alexander Sax</strong>,
                  <a href="http://www.cs.stanford.edu/~amirz/">Amir Zamir</a>,
                  <a href="https://geometry.stanford.edu/member/guibas/">Leonidas Guibas</a>,
                  <a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a>
                  <br>
                  <em>ECCV</em>, 2020 &nbsp <font color="red"><strong>(Spotlight)</strong></font>
                  <br>
                  <a href="https://sidetuning.berkeley.edu/">project page</a> /
                  <a href="https://arxiv.org/abs/1912.13503">arXiv</a> /
                  <a href="https://github.com/jozhang97/side-tuning">github</a>
                  <p></p>
                  <p>
                    Compared to full fine-tuning, and other parameter-efficient fine-tuning methods, simply adding a lightweight side network to control the activations is surprisingly competitive.
                    We show results across vision, language, and robotics using both supervised learning and behavior cloning.
                  </p>
                </td>  
              </tr>

              <tr onmouseout="midlevel_manipulation_stop()" onmouseover="midlevel_manipulation_start()">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center; height: 100%;">
                    <div class="two" id='midlevel_manipulation_image' style="display: flex; align-items: center;">
                      <img src="images/midlevel_manip_two.gif" width=100%>
                    </div>
                    <img src='images/midlevel_manip_one.jpg' width=100%>
                  </div>
                  <script type="text/javascript">
                    function midlevel_manipulation_start() {
                      document.getElementById('midlevel_manipulation_image').style.opacity = "1";
                    }

                    function midlevel_manipulation_stop() {
                      document.getElementById('midlevel_manipulation_image').style.opacity = "0";
                    }
                    midlevel_manipulation_stop()
                  </script>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://sites.google.com/view/mid-level-representations/home">
                    <span class="papertitle">Robust Policies via Mid-Level Visual Representations: An Experimental Study in Manipulation and Navigation</span>
                  </a>
                  <br>
                  <a href="https://bycn.github.io/">Bryan Chen*</a>,
                  <strong>Alexander Sax*</strong>,
                  <a href="">Francis E. Lewis</a>,
                  <a href="https://www.salesforce.com/blog/author/silvio-savarese/">Silvio Savarese</a>,
                  <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>,
                  <a href="https://vilab.epfl.ch/zamir/">Amir Zamir</a>,
                  <a href="https://www.cs.cmu.edu/~lerrelp/">Lerrel Pinto</a>
                  <br>
                  <em>CoRL</em>, 2020
                  <br>
                  <a href="https://sites.google.com/view/mid-level-representations/home">project page</a> /
                  <a href="https://arxiv.org/abs/2011.06698">arXiv</a> /
                  <a href="https://github.com/alexsax/robust-policies-via-midlevel-vision">github</a>
                  <p></p>
                  <p>
                    Compared to domain randomization, pretrained image representations yield qualitatively better generalization to new environments, including sim-to-real transfer.
                    In data regimes where visual domain randomization gets near-zero training performance, policies using pretrained image representations can saturate the benchmark.
                  </p>
                </td>
              </tr>
          

              <tr onmouseout="midlevel_stop()" onmouseover="midlevel_start()">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center; height: 100%;">
                    <div class="two" id='midlevel_image' style="display: flex; align-items: center;">
                      <video width=100% muted autoplay loop>
                        <source src="images/midlevel_two.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video>
                    </div>
                    <img src='images/midlevel_one.jpg' width=100%>
                  </div>
                  <script type="text/javascript">
                    function midlevel_start() {
                      document.getElementById('midlevel_image').style.opacity = "1";
                    }

                    function midlevel_stop() {
                      document.getElementById('midlevel_image').style.opacity = "0";
                    }
                    midlevel_stop()
                  </script>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://perceptual.actor">
                    <span class="papertitle">Learning to Navigate Using Mid-Level Visual Priors</span>
                  </a>
                  <br>
                  <strong>Alexander Sax*</strong>,
                  <a href="https://jozhang97.github.io/">Jeffrey O. Zhang*</a>,
                  <a href="">Bradley Emi</a>,
                  <a href="https://vilab.epfl.ch/zamir/">Amir Zamir</a>,
                  <a href="https://geometry.stanford.edu/member/guibas/">Leonidas Guibas</a>,
                  <a href="https://www.salesforce.com/blog/author/silvio-savarese/">Silvio Savarese</a>,
                  <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>
                  <br>
                  <em>CoRL</em>, 2019 &nbsp <font color="red"><strong>(Winner of CVPR19 Habitat Challenge RGB Track)</strong></font>
                  <br>
                  <a href="https://perceptual.actor">project page</a> /
                  <a href="https://arxiv.org/abs/1812.11971">arXiv</a> /
                  <a href="https://github.com/alexsax/midlevel-reps">github</a>
                  <p></p>
                  <p>
                    RL agents with pretrained image representations require 10x less data to achieve a given performance vs. starting from a random initialization.
                    <!-- Features with stronger geometric and segmentation supervision outperformed imagenet features and autoencoders. -->
                  </p>
                </td>
              </tr>



              <tr onmouseout="taskonomy_stop()" onmouseover="taskonomy_start()" bgcolor="#ffffd0">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center; height: 100%;">
                    <div class="two" id='taskonomy_image'>
                      <video width=100% muted autoplay loop>
                        <source src="images/taskonomy_two.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/taskonomy_one.png' width=100%>
                  </div>
                  <script type="text/javascript">
                    function taskonomy_start() {
                      document.getElementById('taskonomy_image').style.opacity = "1";
                    }

                    function taskonomy_stop() {
                      document.getElementById('taskonomy_image').style.opacity = "0";
                    }
                    taskonomy_stop()
                  </script>
                </td> 
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="http://taskonomy.stanford.edu/">
                    <span class="papertitle">Taskonomy: Disentangling Task Transfer Learning</span>
                  </a>
                  <br>
                  <a href="https://vilab.epfl.ch/zamir/">Amir Zamir</a>,
                  <strong>Alexander Sax*</strong>,
                  <a href="">William B. Shen*</a>,
                  <a href="https://geometry.stanford.edu/member/guibas/">Leonidas Guibas</a>,
                  <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>,
                  <a href="https://www.salesforce.com/blog/author/silvio-savarese/">Silvio Savarese</a>
                  <br>
                  <em>CVPR</em>, 2018 &nbsp <font color="red"><strong>(Best Paper Award)</strong></font>
                  <br>
                  <a href="http://taskonomy.stanford.edu/">project page</a> /
                  <a href="https://arxiv.org/abs/1804.08328">arXiv</a> /
                  <a href="https://github.com/StanfordVL/taskonomy">github</a>
                  <p></p>
                  <p>
                    A fully computational approach for modeling relationships across 26 fundamental vision tasks. 
                    Exploiting this structure with a pretraining/finetuning learning curriculum reduces overall supervision required by ~2/3.
                  </p>
                </td>
              </tr>

              <tr onmouseout="gibson_stop()" onmouseover="gibson_start()">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='gibson_image'><video width=100% muted autoplay loop>
                        <source src="images/gibson.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                      </video></div>
                    <img src='images/gibson.jpg' width=100%>
                  </div>
                  <script type="text/javascript">
                    function gibson_start() {
                      document.getElementById('gibson_image').style.opacity = "1";
                    }

                    function gibson_stop() {
                      document.getElementById('gibson_image').style.opacity = "0";
                    }
                    gibson_stop()
                  </script>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="http://gibsonenv.stanford.edu/">
                    <span class="papertitle">Gibson Env: Real-World Perception for Embodied Agents</span>
                  </a>
                  <br>
                  <a href="">Zhi-Yang He*</a>,
                  <a href="">Fei Xia*</a>,
                  <a href="https://vilab.epfl.ch/zamir/">Amir Zamir*</a>,
                  <strong>Alexander Sax</strong>,
                  <a href="http://www.cs.berkeley.edu/~malik/">Jitendra Malik</a>,
                  <a href="https://www.salesforce.com/blog/author/silvio-savarese/">Silvio Savarese</a>
                  <br>
                  <em>CVPR</em>, 2018 &nbsp <font color="red"><strong>(Spotlight, NVIDIA Pioneering Research Award)</strong></font>
                  <br>
                  <a href="http://gibsonenv.stanford.edu/">project page</a> /
                  <a href="https://arxiv.org/abs/1808.10654">arXiv</a> /
                  <a href="https://github.com/StanfordVL/GibsonEnv">github</a>
                  <p></p>
                  <p>
                    A rendering and physics simulator to bridge the gap between large-scale simulation and real-world environments.
                    The simulator features a database of thousands of real spaces and uses a generative model (GAN) for super-sampling.
                  </p>
                </td>
              </tr>


              <tr onmouseout="semantics2d3d_stop()" onmouseover="semantics2d3d_start()">
                <td style="padding:16px;width:20%;vertical-align:middle">
                  <div class="one" style="display: flex; align-items: center; height: 100%;">
                    <div class="two" id='semantics2d3d_image'><img src="images/2d3ds_two.jpg" width=100%></div>
                    <img src='images/2d3ds_one.jpg' width=100%>
                  </div>
                  <script type="text/javascript">
                    function semantics2d3d_start() {
                      document.getElementById('semantics2d3d_image').style.opacity = "1";
                    }

                    function semantics2d3d_stop() {
                      document.getElementById('semantics2d3d_image').style.opacity = "0";
                    }
                    semantics2d3d_stop()
                  </script>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://3dsemantics.stanford.edu/">
                    <span class="papertitle">Joint 2D-3D-Semantic Data for Indoor Scene Understanding</span>
                  </a>
                  <br>
                  <a href="">Iro Armeni*</a>,
                  <strong>Alexander Sax*</strong>,
                  <a href="https://vilab.epfl.ch/zamir/">Amir R. Zamir</a>,
                  <a href="https://www.salesforce.com/blog/author/silvio-savarese/">Silvio Savarese</a>
                  <br>
                  <em>arXiv</em>, 2017
                  <br>
                  <a href="https://github.com/alexsax/2D-3D-Semantics">project page</a> /
                  <a href="https://arxiv.org/abs/1702.01105">arXiv</a> /
                  <a href="https://github.com/alexsax/2D-3D-Semantics">github</a>
                  <p></p>
                  <p>
                    A large-scale indoor dataset providing mutually registered 2D, 2.5D and 3D modalities with instance-level semantic annotations. 
                    Covers over 6,000m² across 6 large-scale indoor areas with 70,000+ RGB images and corresponding poses, depths, surface normals, semantic annotations, and pointmaps.
                  </p>
                </td>
              </tr>

            </tbody>
          </table>


          <table style="width:100%; margin:0 auto; border:0; border-spacing:0; padding:16px;">
            <tbody>
              <tr>
                <td>
                  <h2>Misc.</h2>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>

<!-- 
              <tr>
                <td align="center" style="padding:16px;width:20%;vertical-align:middle">
                  <div class="colored-box" style="background-color: #fcb97d;">
                    <h2>Micropapers</h2>
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:middle">
                  <a href="https://arxiv.org/abs/2112.11687">Squareplus: A Softplus-Like Algebraic Rectifier</a>
                  <br>
                  <a href="https://arxiv.org/abs/2010.09714">A Convenient Generalization of Schlick's Bias and Gain
                    Functions</a>
                  <br>
                  <a href="https://arxiv.org/abs/1704.07483">Continuously Differentiable Exponential Linear Units</a>
                  <br>
                  <a href="https://jonbarron.info/data/cvpr2023_llm_workshop_annotated.pdf">Scholars & Big Models: How
                    Can Academics Adapt?</a>
                </td>
              </tr> -->


              <!-- <tr>
                <td align="center" style="padding:16px;width:20%;vertical-align:middle">
                  <div class="colored-box" style="background-color: #aaba9e;">
                    <h2>Recorded Talks</h2>
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:center">
                  <a href="https://www.youtube.com/watch?v=h9vq_65eDas">View Dependent Podcast, 2024</a>
                  <br>
                  <a href="https://www.youtube.com/watch?v=4tDhYsFuEqo">Bay Area Robotics Symposium, 2023
                  </a>
                  <br>
                  <a href="https://youtu.be/TvWkwDYtBP4?t=7604">EGSR Keynote, 2021</a>
                  <br>
                  <a href="https://www.youtube.com/watch?v=nRyOzHpcr4Q">TUM AI Lecture Series, 2020</a>
                  <br>
                  <a href="https://www.youtube.com/watch?v=HfJpQCBTqZs">Vision & Graphics Seminar at MIT, 2020</a>
                </td>
              </tr> -->

              <tr>
                <td align="center" style="padding:16px;width:20%;vertical-align:middle">
                  <div class="colored-box" style="background-color: #c6b89e;">
                    <h2>Academic Service</h2>
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:center">
                  <a href="https://bair.berkeley.edu/">Graduate Mentor: BAIR Undergraduate Mentoring,	2019-2023</a>
                  <br>
                  <a href="">Graduate Admissions (Berkeley 2x, EPFL 1x)</a>
                  <br>
                  <a href="http://3dv.stanford.edu/people.html">Student Organizer, 3DV 2016</a>
                  <br>
                  <a href="https://www.assu.stanford.edu/branches/class-presidents">Stanford Junior Class President, 2014-2015</a>
                  <!-- <br>
                  <a href="https://cvpr2022.thecvf.com/area-chairs">Area Chair, CVPR 2022</a>
                  <br>
                  <a href="http://cvpr2021.thecvf.com/area-chairs">Area Chair & Award Committee Member, CVPR 2021</a>
                  <br>
                  <a href="http://cvpr2019.thecvf.com/area_chairs">Area Chair, CVPR 2019</a> -->
                  <br>
                </td>
              </tr>



              <tr>
                <td align="center" style="padding:16px;width:20%;vertical-align:middle">
                  <div class="colored-box" style="background-color: #edd892;">
                    <h2>Teaching</h2>
                  </div>
                </td>
                <td style="padding:8px;width:80%;vertical-align:center">
                  <a href="https://eecs189.org/">Graduate Student Instructor,
                    Berkeley CS 189/289A 2020 (Machine Learning)</a>
                  <br>
                  <a href="https://web.stanford.edu/class/cs331b/2017/index.html">Course TA,
                    Stanford CS331B (Representation Learning) Fall 2017</a>
                  <br>
                  <a href="https://web.stanford.edu/class/cs103/">Teaching Assistant, 
                    Stanford CS103 (Mathematical Foundations of Computing) 2015</a>
                </td>
              </tr>




            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody> 
              <tr>
                <td style="padding:0px">
                  <br>
                  <p style="text-align:right;font-size:small;">
                    Thanks <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a> for the clean website.
                    <br>
                    Last updated November 2025.
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>